{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJp5jAMqZLW4"
   },
   "source": [
    "# Transformer Text Generation\n",
    "\n",
    "This notebook implements a simple word level neural network. Based on the the text, we will train an Transformer to predict the next words in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12926,
     "status": "ok",
     "timestamp": 1730560036480,
     "user": {
      "displayName": "Thomas Long",
      "userId": "15975061006049740720"
     },
     "user_tz": 240
    },
    "id": "cfL90JxzPsvc",
    "outputId": "3b57c229-61a3-4690-8320-71007ed25c83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\khale\\anaconda3\\lib\\site-packages (from sacremoses) (2023.10.3)\n",
      "Requirement already satisfied: click in c:\\users\\khale\\anaconda3\\lib\\site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\khale\\anaconda3\\lib\\site-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\khale\\anaconda3\\lib\\site-packages (from sacremoses) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\khale\\anaconda3\\lib\\site-packages (from click->sacremoses) (0.4.6)\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "   ---------------------------------------- 0.0/897.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/897.5 kB ? eta -:--:--\n",
      "   - ------------------------------------- 41.0/897.5 kB 653.6 kB/s eta 0:00:02\n",
      "   -- ------------------------------------ 61.4/897.5 kB 825.8 kB/s eta 0:00:02\n",
      "   -- ------------------------------------ 61.4/897.5 kB 825.8 kB/s eta 0:00:02\n",
      "   -- ------------------------------------ 61.4/897.5 kB 825.8 kB/s eta 0:00:02\n",
      "   -- ------------------------------------ 61.4/897.5 kB 825.8 kB/s eta 0:00:02\n",
      "   --- ----------------------------------- 71.7/897.5 kB 262.6 kB/s eta 0:00:04\n",
      "   --- ----------------------------------- 71.7/897.5 kB 262.6 kB/s eta 0:00:04\n",
      "   --- ----------------------------------- 71.7/897.5 kB 262.6 kB/s eta 0:00:04\n",
      "   ---- ---------------------------------- 92.2/897.5 kB 180.8 kB/s eta 0:00:05\n",
      "   ---- ---------------------------------- 92.2/897.5 kB 180.8 kB/s eta 0:00:05\n",
      "   ---- ---------------------------------- 92.2/897.5 kB 180.8 kB/s eta 0:00:05\n",
      "   ---- ---------------------------------- 92.2/897.5 kB 180.8 kB/s eta 0:00:05\n",
      "   ---- --------------------------------- 102.4/897.5 kB 140.5 kB/s eta 0:00:06\n",
      "   ---- --------------------------------- 102.4/897.5 kB 140.5 kB/s eta 0:00:06\n",
      "   ---- --------------------------------- 102.4/897.5 kB 140.5 kB/s eta 0:00:06\n",
      "   ---- --------------------------------- 102.4/897.5 kB 140.5 kB/s eta 0:00:06\n",
      "   ---- --------------------------------- 102.4/897.5 kB 140.5 kB/s eta 0:00:06\n",
      "   ----- -------------------------------- 122.9/897.5 kB 124.3 kB/s eta 0:00:07\n",
      "   ----- -------------------------------- 122.9/897.5 kB 124.3 kB/s eta 0:00:07\n",
      "   ----- -------------------------------- 122.9/897.5 kB 124.3 kB/s eta 0:00:07\n",
      "   ----- -------------------------------- 122.9/897.5 kB 124.3 kB/s eta 0:00:07\n",
      "   ----- -------------------------------- 122.9/897.5 kB 124.3 kB/s eta 0:00:07\n",
      "   ------ ------------------------------- 143.4/897.5 kB 116.8 kB/s eta 0:00:07\n",
      "   ------ ------------------------------- 143.4/897.5 kB 116.8 kB/s eta 0:00:07\n",
      "   ------ ------------------------------- 143.4/897.5 kB 116.8 kB/s eta 0:00:07\n",
      "   ------ ------------------------------- 153.6/897.5 kB 108.0 kB/s eta 0:00:07\n",
      "   ------ ------------------------------- 153.6/897.5 kB 108.0 kB/s eta 0:00:07\n",
      "   ------ ------------------------------- 153.6/897.5 kB 108.0 kB/s eta 0:00:07\n",
      "   ------ ------------------------------- 153.6/897.5 kB 108.0 kB/s eta 0:00:07\n",
      "   ------ ------------------------------- 153.6/897.5 kB 108.0 kB/s eta 0:00:07\n",
      "   ------ ------------------------------- 153.6/897.5 kB 108.0 kB/s eta 0:00:07\n",
      "   ------ ------------------------------- 153.6/897.5 kB 108.0 kB/s eta 0:00:07\n",
      "   ------- ------------------------------- 174.1/897.5 kB 98.9 kB/s eta 0:00:08\n",
      "   ------- ------------------------------- 174.1/897.5 kB 98.9 kB/s eta 0:00:08\n",
      "   ------- ------------------------------- 174.1/897.5 kB 98.9 kB/s eta 0:00:08\n",
      "   ------- ------------------------------- 174.1/897.5 kB 98.9 kB/s eta 0:00:08\n",
      "   ------- ------------------------------- 174.1/897.5 kB 98.9 kB/s eta 0:00:08\n",
      "   -------- ------------------------------ 184.3/897.5 kB 92.1 kB/s eta 0:00:08\n",
      "   -------- ------------------------------ 184.3/897.5 kB 92.1 kB/s eta 0:00:08\n",
      "   -------- ------------------------------ 184.3/897.5 kB 92.1 kB/s eta 0:00:08\n",
      "   -------- ------------------------------ 184.3/897.5 kB 92.1 kB/s eta 0:00:08\n",
      "   -------- ------------------------------ 204.8/897.5 kB 91.6 kB/s eta 0:00:08\n",
      "   -------- ------------------------------ 204.8/897.5 kB 91.6 kB/s eta 0:00:08\n",
      "   -------- ------------------------------ 204.8/897.5 kB 91.6 kB/s eta 0:00:08\n",
      "   -------- ------------------------------ 204.8/897.5 kB 91.6 kB/s eta 0:00:08\n",
      "   -------- ------------------------------ 204.8/897.5 kB 91.6 kB/s eta 0:00:08\n",
      "   --------- ----------------------------- 225.3/897.5 kB 92.4 kB/s eta 0:00:08\n",
      "   --------- ----------------------------- 225.3/897.5 kB 92.4 kB/s eta 0:00:08\n",
      "   --------- ----------------------------- 225.3/897.5 kB 92.4 kB/s eta 0:00:08\n",
      "   --------- ----------------------------- 225.3/897.5 kB 92.4 kB/s eta 0:00:08\n",
      "   ---------- ---------------------------- 235.5/897.5 kB 89.6 kB/s eta 0:00:08\n",
      "   ---------- ---------------------------- 235.5/897.5 kB 89.6 kB/s eta 0:00:08\n",
      "   ---------- ---------------------------- 235.5/897.5 kB 89.6 kB/s eta 0:00:08\n",
      "   ---------- ---------------------------- 235.5/897.5 kB 89.6 kB/s eta 0:00:08\n",
      "   ---------- ---------------------------- 235.5/897.5 kB 89.6 kB/s eta 0:00:08\n",
      "   ----------- --------------------------- 256.0/897.5 kB 87.4 kB/s eta 0:00:08\n",
      "   ----------- --------------------------- 256.0/897.5 kB 87.4 kB/s eta 0:00:08\n",
      "   ----------- --------------------------- 256.0/897.5 kB 87.4 kB/s eta 0:00:08\n",
      "   ----------- --------------------------- 256.0/897.5 kB 87.4 kB/s eta 0:00:08\n",
      "   ----------- --------------------------- 256.0/897.5 kB 87.4 kB/s eta 0:00:08\n",
      "   ----------- --------------------------- 266.2/897.5 kB 84.0 kB/s eta 0:00:08\n",
      "   ----------- --------------------------- 266.2/897.5 kB 84.0 kB/s eta 0:00:08\n",
      "   ----------- --------------------------- 266.2/897.5 kB 84.0 kB/s eta 0:00:08\n",
      "   ----------- --------------------------- 266.2/897.5 kB 84.0 kB/s eta 0:00:08\n",
      "   ----------- --------------------------- 266.2/897.5 kB 84.0 kB/s eta 0:00:08\n",
      "   ------------ -------------------------- 286.7/897.5 kB 83.5 kB/s eta 0:00:08\n",
      "   ------------ -------------------------- 286.7/897.5 kB 83.5 kB/s eta 0:00:08\n",
      "   ------------ -------------------------- 286.7/897.5 kB 83.5 kB/s eta 0:00:08\n",
      "   ------------ -------------------------- 286.7/897.5 kB 83.5 kB/s eta 0:00:08\n",
      "   ------------ -------------------------- 286.7/897.5 kB 83.5 kB/s eta 0:00:08\n",
      "   ------------- ------------------------- 307.2/897.5 kB 84.5 kB/s eta 0:00:07\n",
      "   ------------- ------------------------- 307.2/897.5 kB 84.5 kB/s eta 0:00:07\n",
      "   ------------- ------------------------- 307.2/897.5 kB 84.5 kB/s eta 0:00:07\n",
      "   ------------- ------------------------- 307.2/897.5 kB 84.5 kB/s eta 0:00:07\n",
      "   ------------- ------------------------- 317.4/897.5 kB 81.9 kB/s eta 0:00:08\n",
      "   ------------- ------------------------- 317.4/897.5 kB 81.9 kB/s eta 0:00:08\n",
      "   ------------- ------------------------- 317.4/897.5 kB 81.9 kB/s eta 0:00:08\n",
      "   ------------- ------------------------- 317.4/897.5 kB 81.9 kB/s eta 0:00:08\n",
      "   ------------- ------------------------- 317.4/897.5 kB 81.9 kB/s eta 0:00:08\n",
      "   -------------- ------------------------ 337.9/897.5 kB 81.6 kB/s eta 0:00:07\n",
      "   -------------- ------------------------ 337.9/897.5 kB 81.6 kB/s eta 0:00:07\n",
      "   -------------- ------------------------ 337.9/897.5 kB 81.6 kB/s eta 0:00:07\n",
      "   -------------- ------------------------ 337.9/897.5 kB 81.6 kB/s eta 0:00:07\n",
      "   -------------- ------------------------ 337.9/897.5 kB 81.6 kB/s eta 0:00:07\n",
      "   --------------- ----------------------- 348.2/897.5 kB 79.8 kB/s eta 0:00:07\n",
      "   --------------- ----------------------- 348.2/897.5 kB 79.8 kB/s eta 0:00:07\n",
      "   --------------- ----------------------- 348.2/897.5 kB 79.8 kB/s eta 0:00:07\n",
      "   --------------- ----------------------- 348.2/897.5 kB 79.8 kB/s eta 0:00:07\n",
      "   ---------------- ---------------------- 368.6/897.5 kB 81.0 kB/s eta 0:00:07\n",
      "   ---------------- ---------------------- 368.6/897.5 kB 81.0 kB/s eta 0:00:07\n",
      "   ---------------- ---------------------- 368.6/897.5 kB 81.0 kB/s eta 0:00:07\n",
      "   ---------------- ---------------------- 368.6/897.5 kB 81.0 kB/s eta 0:00:07\n",
      "   ---------------- ---------------------- 368.6/897.5 kB 81.0 kB/s eta 0:00:07\n",
      "   ---------------- ---------------------- 389.1/897.5 kB 81.1 kB/s eta 0:00:07\n",
      "   ---------------- ---------------------- 389.1/897.5 kB 81.1 kB/s eta 0:00:07\n",
      "   ---------------- ---------------------- 389.1/897.5 kB 81.1 kB/s eta 0:00:07\n",
      "   ---------------- ---------------------- 389.1/897.5 kB 81.1 kB/s eta 0:00:07\n",
      "   ----------------- --------------------- 399.4/897.5 kB 80.1 kB/s eta 0:00:07\n",
      "   ----------------- --------------------- 399.4/897.5 kB 80.1 kB/s eta 0:00:07\n",
      "   ----------------- --------------------- 399.4/897.5 kB 80.1 kB/s eta 0:00:07\n",
      "   ----------------- --------------------- 399.4/897.5 kB 80.1 kB/s eta 0:00:07\n",
      "   ----------------- --------------------- 399.4/897.5 kB 80.1 kB/s eta 0:00:07\n",
      "   ------------------ -------------------- 419.8/897.5 kB 79.7 kB/s eta 0:00:06\n",
      "   ------------------ -------------------- 419.8/897.5 kB 79.7 kB/s eta 0:00:06\n",
      "   ------------------ -------------------- 419.8/897.5 kB 79.7 kB/s eta 0:00:06\n",
      "   ------------------ -------------------- 419.8/897.5 kB 79.7 kB/s eta 0:00:06\n",
      "   ------------------ -------------------- 419.8/897.5 kB 79.7 kB/s eta 0:00:06\n",
      "   ------------------ -------------------- 430.1/897.5 kB 78.3 kB/s eta 0:00:06\n",
      "   ------------------ -------------------- 430.1/897.5 kB 78.3 kB/s eta 0:00:06\n",
      "   ------------------ -------------------- 430.1/897.5 kB 78.3 kB/s eta 0:00:06\n",
      "   ------------------ -------------------- 430.1/897.5 kB 78.3 kB/s eta 0:00:06\n",
      "   ------------------- ------------------- 450.6/897.5 kB 79.2 kB/s eta 0:00:06\n",
      "   ------------------- ------------------- 450.6/897.5 kB 79.2 kB/s eta 0:00:06\n",
      "   ------------------- ------------------- 450.6/897.5 kB 79.2 kB/s eta 0:00:06\n",
      "   ------------------- ------------------- 450.6/897.5 kB 79.2 kB/s eta 0:00:06\n",
      "   ------------------- ------------------- 450.6/897.5 kB 79.2 kB/s eta 0:00:06\n",
      "   ------------------- ------------------- 450.6/897.5 kB 79.2 kB/s eta 0:00:06\n",
      "   -------------------- ------------------ 471.0/897.5 kB 78.2 kB/s eta 0:00:06\n",
      "   -------------------- ------------------ 471.0/897.5 kB 78.2 kB/s eta 0:00:06\n",
      "   -------------------- ------------------ 471.0/897.5 kB 78.2 kB/s eta 0:00:06\n",
      "   -------------------- ------------------ 471.0/897.5 kB 78.2 kB/s eta 0:00:06\n",
      "   -------------------- ------------------ 481.3/897.5 kB 77.7 kB/s eta 0:00:06\n",
      "   -------------------- ------------------ 481.3/897.5 kB 77.7 kB/s eta 0:00:06\n",
      "   -------------------- ------------------ 481.3/897.5 kB 77.7 kB/s eta 0:00:06\n",
      "   -------------------- ------------------ 481.3/897.5 kB 77.7 kB/s eta 0:00:06\n",
      "   -------------------- ------------------ 481.3/897.5 kB 77.7 kB/s eta 0:00:06\n",
      "   --------------------- ----------------- 501.8/897.5 kB 78.1 kB/s eta 0:00:06\n",
      "   --------------------- ----------------- 501.8/897.5 kB 78.1 kB/s eta 0:00:06\n",
      "   --------------------- ----------------- 501.8/897.5 kB 78.1 kB/s eta 0:00:06\n",
      "   --------------------- ----------------- 501.8/897.5 kB 78.1 kB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 512.0/897.5 kB 76.8 kB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 512.0/897.5 kB 76.8 kB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 512.0/897.5 kB 76.8 kB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 512.0/897.5 kB 76.8 kB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 512.0/897.5 kB 76.8 kB/s eta 0:00:06\n",
      "   ---------------------- ---------------- 512.0/897.5 kB 76.8 kB/s eta 0:00:06\n",
      "   ----------------------- --------------- 532.5/897.5 kB 77.0 kB/s eta 0:00:05\n",
      "   ----------------------- --------------- 532.5/897.5 kB 77.0 kB/s eta 0:00:05\n",
      "   ----------------------- --------------- 532.5/897.5 kB 77.0 kB/s eta 0:00:05\n",
      "   ------------------------ -------------- 553.0/897.5 kB 78.1 kB/s eta 0:00:05\n",
      "   ------------------------ -------------- 553.0/897.5 kB 78.1 kB/s eta 0:00:05\n",
      "   ------------------------ -------------- 553.0/897.5 kB 78.1 kB/s eta 0:00:05\n",
      "   ------------------------ -------------- 553.0/897.5 kB 78.1 kB/s eta 0:00:05\n",
      "   ------------------------ -------------- 553.0/897.5 kB 78.1 kB/s eta 0:00:05\n",
      "   ------------------------ -------------- 553.0/897.5 kB 78.1 kB/s eta 0:00:05\n",
      "   ------------------------ -------------- 563.2/897.5 kB 76.6 kB/s eta 0:00:05\n",
      "   ------------------------ -------------- 563.2/897.5 kB 76.6 kB/s eta 0:00:05\n",
      "   ------------------------ -------------- 563.2/897.5 kB 76.6 kB/s eta 0:00:05\n",
      "   ------------------------ -------------- 563.2/897.5 kB 76.6 kB/s eta 0:00:05\n",
      "   ------------------------- ------------- 583.7/897.5 kB 77.1 kB/s eta 0:00:05\n",
      "   ------------------------- ------------- 583.7/897.5 kB 77.1 kB/s eta 0:00:05\n",
      "   ------------------------- ------------- 583.7/897.5 kB 77.1 kB/s eta 0:00:05\n",
      "   ------------------------- ------------- 583.7/897.5 kB 77.1 kB/s eta 0:00:05\n",
      "   ------------------------- ------------- 583.7/897.5 kB 77.1 kB/s eta 0:00:05\n",
      "   ------------------------- ------------- 593.9/897.5 kB 76.2 kB/s eta 0:00:04\n",
      "   ------------------------- ------------- 593.9/897.5 kB 76.2 kB/s eta 0:00:04\n",
      "   ------------------------- ------------- 593.9/897.5 kB 76.2 kB/s eta 0:00:04\n",
      "   ------------------------- ------------- 593.9/897.5 kB 76.2 kB/s eta 0:00:04\n",
      "   ------------------------- ------------- 593.9/897.5 kB 76.2 kB/s eta 0:00:04\n",
      "   ------------------------- ------------- 593.9/897.5 kB 76.2 kB/s eta 0:00:04\n",
      "   -------------------------- ------------ 614.4/897.5 kB 75.8 kB/s eta 0:00:04\n",
      "   -------------------------- ------------ 614.4/897.5 kB 75.8 kB/s eta 0:00:04\n",
      "   -------------------------- ------------ 614.4/897.5 kB 75.8 kB/s eta 0:00:04\n",
      "   -------------------------- ------------ 614.4/897.5 kB 75.8 kB/s eta 0:00:04\n",
      "   -------------------------- ------------ 614.4/897.5 kB 75.8 kB/s eta 0:00:04\n",
      "   -------------------------- ------------ 614.4/897.5 kB 75.8 kB/s eta 0:00:04\n",
      "   --------------------------- ----------- 634.9/897.5 kB 75.9 kB/s eta 0:00:04\n",
      "   --------------------------- ----------- 634.9/897.5 kB 75.9 kB/s eta 0:00:04\n",
      "   --------------------------- ----------- 634.9/897.5 kB 75.9 kB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 645.1/897.5 kB 75.5 kB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 645.1/897.5 kB 75.5 kB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 645.1/897.5 kB 75.5 kB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 645.1/897.5 kB 75.5 kB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 645.1/897.5 kB 75.5 kB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 665.6/897.5 kB 75.7 kB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 665.6/897.5 kB 75.7 kB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 665.6/897.5 kB 75.7 kB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 665.6/897.5 kB 75.7 kB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 665.6/897.5 kB 75.7 kB/s eta 0:00:04\n",
      "   ----------------------------- --------- 675.8/897.5 kB 75.0 kB/s eta 0:00:03\n",
      "   ----------------------------- --------- 675.8/897.5 kB 75.0 kB/s eta 0:00:03\n",
      "   ----------------------------- --------- 675.8/897.5 kB 75.0 kB/s eta 0:00:03\n",
      "   ----------------------------- --------- 675.8/897.5 kB 75.0 kB/s eta 0:00:03\n",
      "   ----------------------------- --------- 675.8/897.5 kB 75.0 kB/s eta 0:00:03\n",
      "   ------------------------------ -------- 696.3/897.5 kB 75.4 kB/s eta 0:00:03\n",
      "   ------------------------------ -------- 696.3/897.5 kB 75.4 kB/s eta 0:00:03\n",
      "   ------------------------------ -------- 696.3/897.5 kB 75.4 kB/s eta 0:00:03\n",
      "   ------------------------------ -------- 696.3/897.5 kB 75.4 kB/s eta 0:00:03\n",
      "   ------------------------------ -------- 696.3/897.5 kB 75.4 kB/s eta 0:00:03\n",
      "   ------------------------------- ------- 716.8/897.5 kB 75.4 kB/s eta 0:00:03\n",
      "   ------------------------------- ------- 716.8/897.5 kB 75.4 kB/s eta 0:00:03\n",
      "   ------------------------------- ------- 716.8/897.5 kB 75.4 kB/s eta 0:00:03\n",
      "   ------------------------------- ------- 716.8/897.5 kB 75.4 kB/s eta 0:00:03\n",
      "   ------------------------------- ------- 716.8/897.5 kB 75.4 kB/s eta 0:00:03\n",
      "   ------------------------------- ------- 716.8/897.5 kB 75.4 kB/s eta 0:00:03\n",
      "   ------------------------------- ------- 727.0/897.5 kB 74.4 kB/s eta 0:00:03\n",
      "   ------------------------------- ------- 727.0/897.5 kB 74.4 kB/s eta 0:00:03\n",
      "   ------------------------------- ------- 727.0/897.5 kB 74.4 kB/s eta 0:00:03\n",
      "   -------------------------------- ------ 747.5/897.5 kB 75.3 kB/s eta 0:00:02\n",
      "   -------------------------------- ------ 747.5/897.5 kB 75.3 kB/s eta 0:00:02\n",
      "   -------------------------------- ------ 747.5/897.5 kB 75.3 kB/s eta 0:00:02\n",
      "   -------------------------------- ------ 747.5/897.5 kB 75.3 kB/s eta 0:00:02\n",
      "   -------------------------------- ------ 757.8/897.5 kB 74.5 kB/s eta 0:00:02\n",
      "   -------------------------------- ------ 757.8/897.5 kB 74.5 kB/s eta 0:00:02\n",
      "   -------------------------------- ------ 757.8/897.5 kB 74.5 kB/s eta 0:00:02\n",
      "   --------------------------------- ----- 778.2/897.5 kB 75.2 kB/s eta 0:00:02\n",
      "   --------------------------------- ----- 778.2/897.5 kB 75.2 kB/s eta 0:00:02\n",
      "   --------------------------------- ----- 778.2/897.5 kB 75.2 kB/s eta 0:00:02\n",
      "   --------------------------------- ----- 778.2/897.5 kB 75.2 kB/s eta 0:00:02\n",
      "   --------------------------------- ----- 778.2/897.5 kB 75.2 kB/s eta 0:00:02\n",
      "   --------------------------------- ----- 778.2/897.5 kB 75.2 kB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 798.7/897.5 kB 75.3 kB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 798.7/897.5 kB 75.3 kB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 798.7/897.5 kB 75.3 kB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 798.7/897.5 kB 75.3 kB/s eta 0:00:02\n",
      "   ----------------------------------- --- 809.0/897.5 kB 74.7 kB/s eta 0:00:02\n",
      "   ----------------------------------- --- 809.0/897.5 kB 74.7 kB/s eta 0:00:02\n",
      "   ----------------------------------- --- 809.0/897.5 kB 74.7 kB/s eta 0:00:02\n",
      "   ----------------------------------- --- 809.0/897.5 kB 74.7 kB/s eta 0:00:02\n",
      "   ------------------------------------ -- 829.4/897.5 kB 74.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ -- 829.4/897.5 kB 74.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ -- 829.4/897.5 kB 74.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ -- 829.4/897.5 kB 74.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ -- 829.4/897.5 kB 74.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ -- 829.4/897.5 kB 74.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ -- 839.7/897.5 kB 73.5 kB/s eta 0:00:01\n",
      "   ------------------------------------ -- 839.7/897.5 kB 73.5 kB/s eta 0:00:01\n",
      "   ------------------------------------ -- 839.7/897.5 kB 73.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- - 860.2/897.5 kB 74.2 kB/s eta 0:00:01\n",
      "   ------------------------------------- - 860.2/897.5 kB 74.2 kB/s eta 0:00:01\n",
      "   ------------------------------------- - 860.2/897.5 kB 74.2 kB/s eta 0:00:01\n",
      "   ------------------------------------- - 860.2/897.5 kB 74.2 kB/s eta 0:00:01\n",
      "   --------------------------------------  880.6/897.5 kB 74.8 kB/s eta 0:00:01\n",
      "   --------------------------------------  880.6/897.5 kB 74.8 kB/s eta 0:00:01\n",
      "   --------------------------------------  880.6/897.5 kB 74.8 kB/s eta 0:00:01\n",
      "   --------------------------------------  880.6/897.5 kB 74.8 kB/s eta 0:00:01\n",
      "   --------------------------------------  880.6/897.5 kB 74.8 kB/s eta 0:00:01\n",
      "   --------------------------------------- 897.5/897.5 kB 74.5 kB/s eta 0:00:00\n",
      "Installing collected packages: sacremoses\n",
      "Successfully installed sacremoses-0.1.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1af6f4c2510>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "!pip install sacremoses\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NztEb3nZYoo"
   },
   "source": [
    "**Download and prepare the data**\n",
    "\n",
    "Prepare the text data by encoding characters as integers. Then divide the text into sequences of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1730560036875,
     "user": {
      "displayName": "Thomas Long",
      "userId": "15975061006049740720"
     },
     "user_tz": 240
    },
    "id": "QgmeL9laP-Ui",
    "outputId": "140892f6-d0d5-423c-a35e-006a5016c766"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE SONNETS\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "From fairest creatures we desire increase,\n",
      "That thereby beauty's rose might never die,\n",
      "But as the riper should by time decease,\n",
      "His tender heir might bear his memory:\n",
      "But thou contracted to thine own bright eyes,\n",
      "Feed'st thy light's flame with self-substantial fuel,\n",
      "Making a famine where abundance lies,\n",
      "Thy self thy foe, to thy sweet self too cruel:\n",
      "Thou that art now the world's fresh ornament,\n",
      "And only herald to the gaudy spring,\n",
      "Within thine own bud buriest thy content,\n",
      "And tender churl mak'st waste in niggarding:\n",
      "Pity the world, or else this glutton be,\n",
      "To eat the world's due, by the grave and thee.\n",
      "\n",
      "When forty winters shall besiege thy brow,\n",
      "And dig deep trenches in thy beauty's field,\n",
      "Thy youth's proud livery so gazed on now,\n",
      "Will be a tattered weed of small worth held:  \n",
      "Then being asked, where all thy beauty lies,\n",
      "Where all the treasure of thy lusty days;\n",
      "To say within thine own deep sunken eyes,\n",
      "Were an all-eating shame, and thriftless praise.\n",
      "How much more praise deserved thy beauty's use,\n",
      "If thou couldst answer 'This fair child of mine\n",
      "Shall sum my count, and make my old excuse'\n",
      "Proving his beauty by succession thine.\n",
      "This were to be new made when thou art old,\n",
      "And see thy blood warm when thou feel'st it cold.\n",
      "\n",
      "Look in thy glass and tell the face thou viewest,\n",
      "Now is the time that face should form another,\n",
      "Whose fresh repair if now thou not renewest,\n",
      "Thou dost beguile the world, unbless some mother.\n",
      "For where is she so fair whose uneared womb\n",
      "Disdains the tillage of thy husbandry?\n",
      "Or who is he so fond will be the tomb,\n",
      "Of his self-love to stop posterity?  \n",
      "Thou art thy mother's glass and she in thee\n",
      "Calls back the lovely April of her prime,\n",
      "So thou through windows of thine age shalt see,\n",
      "Despite of wrinkles this thy golden time.\n",
      "But if thou live remembered not to be,\n",
      "Die single and thine image dies with thee.\n",
      "\n",
      "Unthrifty loveliness why dost thou spend,\n",
      "Upon thy self thy beauty's legacy?\n",
      "Nature's bequest gives nothing but doth lend,\n",
      "\n",
      "['from', 'fairest', 'creatures', 'we', 'desire', 'increase', ',\\n', 'that', 'thereby', \"beauty's\", 'rose', 'might', 'never', 'die', ',\\n', 'but', 'as', 'the', 'riper', 'should', 'by', 'time', 'decease', ',\\n', 'his', 'tender', 'heir', 'might', 'bear', 'his', 'memory', ':\\n', 'but', 'thou', 'contracted', 'to', 'thine', 'own', 'bright', 'eyes', ',\\n', \"feed'st\", 'thy', \"light's\", 'flame', 'with', 'self-substantial', 'fuel', ',\\n', 'making', 'a', 'famine', 'where', 'abundance', 'lies', ',\\n', 'thy', 'self', 'thy', 'foe', ',\\n', 'to', 'thy', 'sweet', 'self', 'too', 'cruel', ':\\n', 'thou', 'that', 'art', 'now', 'the', \"world's\", 'fresh', 'ornament', ',\\n', 'and', 'only', 'herald', 'to', 'the', 'gaudy', 'spring', ',\\n', 'within', 'thine', 'own', 'bud', 'buriest', 'thy', 'content', ',\\n', 'and', 'tender', 'churl', \"mak'st\", 'waste', 'in', 'niggarding', ':\\n', 'pity', 'the', 'world', ',\\n', 'or', 'else', 'this', 'glutton', 'be', ',\\n', 'to', 'eat', 'the', \"world's\", 'due', ',\\n', 'by', 'the', 'grave', 'and', 'thee', '.\\n', 'when', 'forty', 'winters', 'shall', 'besiege', 'thy', 'brow', ',\\n', 'and', 'dig', 'deep', 'trenches', 'in', 'thy', \"beauty's\", 'field', ',\\n', 'thy', \"youth's\", 'proud', 'livery', 'so', 'gazed', 'on', 'now', ',\\n', 'will', 'be', 'a', 'tattered', 'weed', 'of', 'small', 'worth', 'held', ':\\n', 'then', 'being', 'asked', ',\\n', 'where', 'all', 'thy', 'beauty', 'lies', ',\\n', 'where', 'all', 'the', 'treasure', 'of', 'thy', 'lusty', 'days', ';\\n', 'to', 'say', 'within', 'thine', 'own', 'deep', 'sunken', 'eyes', ',\\n', 'were', 'an', 'all-eating', 'shame', ',\\n', 'and', 'thriftless', 'praise', '.\\n', 'how', 'much', 'more', 'praise', 'deserved', 'thy', \"beauty's\", 'use', ',\\n', 'if', 'thou', 'couldst', 'answer', \"'this\", 'fair', 'child', 'of', 'mine', 'shall', 'sum', 'my', 'count', ',\\n', 'and', 'make', 'my', 'old', \"excuse'\", 'proving', 'his', 'beauty', 'by', 'succession', 'thine', '.\\n', 'this', 'were', 'to', 'be', 'new', 'made', 'when', 'thou', 'art', 'old', ',\\n', 'and', 'see', 'thy', 'blood', 'warm', 'when', 'thou', \"feel'st\", 'it', 'cold', '.\\n', 'look', 'in', 'thy', 'glass', 'and', 'tell', 'the', 'face', 'thou', 'viewest', ',\\n', 'now', 'is', 'the', 'time', 'that', 'face', 'should', 'form', 'another', ',\\n', 'whose', 'fresh', 'repair', 'if', 'now', 'thou', 'not', 'renewest', ',\\n', 'thou', 'dost', 'beguile', 'the', 'world', ',\\n', 'unbless', 'some', 'mother', '.\\n', 'for', 'where', 'is', 'she', 'so', 'fair', 'whose', 'uneared', 'womb', 'disdains', 'the', 'tillage', 'of', 'thy', 'husbandry', '?\\n', 'or', 'who', 'is', 'he', 'so', 'fond', 'will', 'be', 'the', 'tomb', ',\\n', 'of', 'his', 'self-love', 'to', 'stop', 'posterity', '?\\n', 'thou', 'art', 'thy', \"mother's\", 'glass', 'and', 'she', 'in', 'thee', 'calls', 'back', 'the', 'lovely', 'april', 'of', 'her', 'prime', ',\\n', 'so', 'thou', 'through', 'windows', 'of', 'thine', 'age', 'shalt', 'see', ',\\n', 'despite', 'of', 'wrinkles', 'this', 'thy', 'golden', 'time', '.\\n', 'but', 'if', 'thou', 'live', 'remembered', 'not', 'to', 'be', ',\\n', 'die', 'single', 'and', 'thine', 'image', 'dies', 'with', 'thee', '.\\n', 'unthrifty', 'loveliness', 'why', 'dost', 'thou', 'spend', ',\\n', 'upon', 'thy', 'self', 'thy', \"beauty's\", 'legacy', '?\\n', \"nature's\", 'bequest', 'gives', 'nothing', 'but', 'doth', 'lend', ',\\n', '']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "# Download the file\n",
    "url = \"https://raw.githubusercontent.com/brunoklein99/deep-learning-notes/refs/heads/master/shakespeare.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text[:2012]\n",
    "print(text)\n",
    "\n",
    "# Tokenize the text via regular expressions and split into the individual words\n",
    "punctuation = r\"[,\\.:;\\?]\"\n",
    "\n",
    "def clean_and_split(text):\n",
    "    cleaned_text = re.sub(punctuation, lambda x: f\" {x.group()}\\n \", text.lower().replace('\\n', ' '))\n",
    "    return re.split(\" +\", cleaned_text)\n",
    "\n",
    "word_list = clean_and_split(text)[5:]  # split into words, starting with the actual 1st word\n",
    "print(word_list)\n",
    "\n",
    "# Create character-to-index and index-to-character mappings\n",
    "vocab = Counter(word_list)\n",
    "vocab = sorted(vocab, key=vocab.get, reverse=True)  # Unique words sorted by frequency in descending order\n",
    "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx_to_word = {i: w for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8quA7C6oDs8D"
   },
   "source": [
    "**Define a DataLoader to manage the text data**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1730560036875,
     "user": {
      "displayName": "Thomas Long",
      "userId": "15975061006049740720"
     },
     "user_tz": 240
    },
    "id": "zVzvbuaFDzoz"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "seq_length = 50 # Length of input sequence for the Transformer\n",
    "batch_size = 64\n",
    "\n",
    "# Create dataset and dataloader (compare to week 5)\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, seq_length):\n",
    "        self.data = [word_to_idx[word] for word in text]\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.data[idx:idx + self.seq_length]\n",
    "        target_seq = self.data[idx + 1:idx + self.seq_length + 1]\n",
    "        return torch.tensor(input_seq), torch.tensor(target_seq)\n",
    "\n",
    "dataset = TextDataset(word_list, seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5snQV4GZo5Y"
   },
   "source": [
    "**Define the Transformer architecture**\n",
    "\n",
    " Use a Transformer and a fully connected layer for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1730560036875,
     "user": {
      "displayName": "Thomas Long",
      "userId": "15975061006049740720"
     },
     "user_tz": 240
    },
    "id": "V5WoFEr3P-sr"
   },
   "outputs": [],
   "source": [
    "class TextTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super(TextTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(1000, d_model)  # Positional embedding (up to 1000 words in a sequence)\n",
    "\n",
    "        # Transformer encoder layers\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers,\n",
    "                                          num_decoder_layers=num_layers, dim_feedforward=dim_feedforward,\n",
    "                                          dropout=dropout, batch_first=True)\n",
    "\n",
    "        # Linear layer to map the transformer output back to the vocabulary\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_length = x.size(1)\n",
    "        positions = torch.arange(0, seq_length).unsqueeze(0)\n",
    "\n",
    "        # Embed input and positions\n",
    "        x = self.embedding(x) + self.pos_embedding(positions)\n",
    "\n",
    "        # Transformer expects (seq_len, batch_size, d_model)\n",
    "        x = x.permute(1, 0, 2)\n",
    "\n",
    "        # Pass through transformer\n",
    "        transformer_out = self.transformer(x, x)\n",
    "\n",
    "        # Map transformer output to vocabulary (to the fully connected layer)\n",
    "        out = self.fc_out(transformer_out.permute(1, 0, 2))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3T_7oxaKZ5hr"
   },
   "source": [
    "**Training Loop**\n",
    "\n",
    "Train the model by feeding sequences and their targets (next wordss) to the RNN.\n",
    "\n",
    "Note that the words are encoded as vectors, with one \"dimension\" per word identified in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 446334,
     "status": "ok",
     "timestamp": 1730560483198,
     "user": {
      "displayName": "Thomas Long",
      "userId": "15975061006049740720"
     },
     "user_tz": 240
    },
    "id": "iBGoop6BQPPc",
    "outputId": "d743c3bd-ecc8-4999-83f8-9675cea4385d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 4.9238\n",
      "Epoch [2/100], Loss: 4.8664\n",
      "Epoch [3/100], Loss: 4.8479\n",
      "Epoch [4/100], Loss: 4.8955\n",
      "Epoch [5/100], Loss: 4.8353\n",
      "Epoch [6/100], Loss: 4.8324\n",
      "Epoch [7/100], Loss: 4.6245\n",
      "Epoch [8/100], Loss: 3.7626\n",
      "Epoch [9/100], Loss: 2.8472\n",
      "Epoch [10/100], Loss: 2.0944\n",
      "Epoch [11/100], Loss: 1.7132\n",
      "Epoch [12/100], Loss: 1.3227\n",
      "Epoch [13/100], Loss: 1.2765\n",
      "Epoch [14/100], Loss: 1.1706\n",
      "Epoch [15/100], Loss: 1.0733\n",
      "Epoch [16/100], Loss: 1.1274\n",
      "Epoch [17/100], Loss: 1.0971\n",
      "Epoch [18/100], Loss: 1.0710\n",
      "Epoch [19/100], Loss: 1.0764\n",
      "Epoch [20/100], Loss: 1.0476\n",
      "Epoch [21/100], Loss: 1.0678\n",
      "Epoch [22/100], Loss: 1.0194\n",
      "Epoch [23/100], Loss: 1.0678\n",
      "Epoch [24/100], Loss: 0.9882\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 100\n",
    "d_model = 128\n",
    "nhead = 8\n",
    "num_layers = 4\n",
    "d_feedforward = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "lr = 0.001\n",
    "acceptable_loss = 1  # Accept the model if the loss is sufficiently low\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = TextTransformer(vocab_size, d_model=d_model, nhead=nhead, num_layers=num_layers,\n",
    "                        dim_feedforward=d_feedforward, dropout=dropout)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "\n",
    "  for X, y in dataloader:\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred.view(-1, vocab_size), y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Accept the model if the loss is sufficiently low\n",
    "    loss_value = loss.item()\n",
    "    if loss_value < acceptable_loss:\n",
    "      print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss_value:.4f}')\n",
    "      break  # exit loop, skip over else clause\n",
    "\n",
    "  else:  # End of the innner loop -- one full epoch\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss_value:.4f}')\n",
    "    continue  # Start the next epoch\n",
    "\n",
    "  break  # Stop after the current epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWcpL1xuX4a3"
   },
   "source": [
    "**Text Generation**\n",
    "\n",
    " Generate text by sampling words from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2091,
     "status": "ok",
     "timestamp": 1730560485281,
     "user": {
      "displayName": "Thomas Long",
      "userId": "15975061006049740720"
     },
     "user_tz": 240
    },
    "id": "nqKbscZ3QWsk",
    "outputId": "7a9cee2b-fbf6-4d26-95ca-eb116d5f4df4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " From fairest creatures we desire mak'st waste all-eating shame face lovely brow ,\n",
      " Where is she in thy light's flame with held :\n",
      " Then nature's ;\n",
      " To the tomb ,\n",
      " To be a tattered weed of his beauty by in niggarding :\n",
      " Thou couldst count spring my back the time that thereby 'this of her besiege thy beauty lies ,\n",
      " Unbless unbless shall sum fresh dig tattered repair beguile face should beauty's field mine shall thereby more praise much his never who is she ,\n",
      " Where all ,\n",
      " Were an all-eating look his the time that face should form another mother livery heir his old\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_text, max_length=10, temperature=1.0):\n",
    "    model.eval()  # Turn-off dropout etc.\n",
    "\n",
    "    # Preprocess the start text\n",
    "    words = clean_and_split(start_text)\n",
    "    input_seq = torch.tensor([word_to_idx[word] for word in words], dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    generated_text = [words[0].capitalize()] + words[1:]\n",
    "    generated_text\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            # Get the model's predictions for the next token\n",
    "            output = model(input_seq)\n",
    "\n",
    "            # Select the last token from the output\n",
    "            next_token_energy = output[0, -1, :]\n",
    "\n",
    "            # Apply temperature (controls randomness)\n",
    "            next_token_energy = next_token_energy / temperature\n",
    "\n",
    "            # Sample the next token from the probability distribution\n",
    "            next_token_probs = torch.softmax(next_token_energy, dim=-1)\n",
    "            next_token = torch.multinomial(next_token_probs, 1).item()\n",
    "\n",
    "            # Add the predicted word to the generated text\n",
    "            word = idx_to_word[next_token]\n",
    "            word = word.capitalize() if generated_text[-1].endswith('\\n') else word\n",
    "            generated_text.append(word)\n",
    "\n",
    "            # Update the input sequence, the input sequence gets fed back to the model in the loop\n",
    "            input_seq = torch.cat([input_seq, torch.tensor([[next_token]], dtype=torch.long)], dim=1)\n",
    "\n",
    "    return ' ' + ' '.join(generated_text)\n",
    "\n",
    "# Generate some Shakespearean-style text\n",
    "start_text = \"From fairest creatures we desire\"\n",
    "generated_sonnet = generate_text(model, start_text, max_length=100, temperature=0.8)\n",
    "print(generated_sonnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1730560485281,
     "user": {
      "displayName": "Thomas Long",
      "userId": "15975061006049740720"
     },
     "user_tz": 240
    },
    "id": "nuSQZU4J3rzQ",
    "outputId": "e38bf70f-9c45-4054-a839-cc13962e548f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the original text :  2 kb.\n",
      "Size of the neural network:  20774 kb.\n"
     ]
    }
   ],
   "source": [
    "def model_size_in_kbytes(model):\n",
    "    total_size = 0\n",
    "    for param in model.parameters():\n",
    "        # Number of elements in the parameter tensor\n",
    "        num_elements = param.numel()\n",
    "\n",
    "        # Number of bytes per element based on the data type (dtype)\n",
    "        bytes_per_element = param.element_size()\n",
    "\n",
    "        # Total size of this parameter\n",
    "        total_size += num_elements * bytes_per_element\n",
    "\n",
    "    return total_size/1000\n",
    "\n",
    "print(f\"Size of the original text : {sys.getsizeof(text)/1000: .0f} kb.\")\n",
    "print(f\"Size of the neural network: {model_size_in_kbytes(model): .0f} kb.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAClCDf0w6Z9"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Using `torch.hub` to load pretrained network**\n",
    "\n",
    "We can use `torch.hub` to load a pretrained network instead of creating our own from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3356,
     "status": "ok",
     "timestamp": 1730560574826,
     "user": {
      "displayName": "Thomas Long",
      "userId": "15975061006049740720"
     },
     "user_tz": 240
    },
    "id": "HAhAVrFfyRKw",
    "outputId": "1ecef474-6584-489d-c871-75a693216482"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khale\\anaconda3\\Lib\\site-packages\\torch\\hub.py:293: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/huggingface/pytorch-transformers/zipball/main\" to C:\\Users\\khale/.cache\\torch\\hub\\main.zip\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained GPT-2 model and tokenizer from Hugging Face using torch.hub\n",
    "model = torch.hub.load('huggingface/pytorch-transformers', 'modelForCausalLM', 'gpt2')\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'gpt2')\n",
    "\n",
    "# Define a prompt (input text)\n",
    "prompt = \"Who was Shakespeare?\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "# Generate text using GPT-2\n",
    "# You can specify `max_length` for the length of the generated text\n",
    "outputs = model.generate(inputs['input_ids'],\n",
    "                         attention_mask=inputs['attention_mask'],\n",
    "                         max_length=35,\n",
    "                         num_return_sequences=1)\n",
    "\n",
    "# Decode the generated text to human-readable format\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 175,
     "status": "ok",
     "timestamp": 1730560679105,
     "user": {
      "displayName": "Thomas Long",
      "userId": "15975061006049740720"
     },
     "user_tz": 240
    },
    "id": "Kc8-Ehxrcu0_",
    "outputId": "2e258d4c-7d3e-45af-c6e7-ceed1230da8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the neural network:  497759 kb.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of the neural network: {model_size_in_kbytes(model): .0f} kb.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPCaANJnmhMUY3+JGxBnf98",
   "provenance": [
    {
     "file_id": "1wQD9F1cltw1Kj2To78fI0wBX9vIpRPhE",
     "timestamp": 1730218292920
    },
    {
     "file_id": "1_CmcST3Bw8xZ0ONfb6xikCMSn8BlAvDC",
     "timestamp": 1729915657410
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
